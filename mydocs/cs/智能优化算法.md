<center><font size=6><strong>智能优化算法</strong></font></center>

# 机器学习（Machine Learning, ML）

## 前言（Preface）

机器学习分为[监督学习](#监督学习)和[无监督学习](#无监督学习)

## 监督学习

简言之，监督学习指数据集中每个样本都经过了标记，根据这些标记做预测。监督学习有两大应用，分类和回归，此文档只对回归进行说明。

### 线性回归（Linear Regression）

**最小二乘法**  
假定一组数据的真实值 y 与测量值 y<sub>i</sub> 之间的误差使用平方来表示

$$\left| y-y_i \right| \to (y-y_i)^2$$

误差的平方和为

$$\sum(y-y_i)^2$$

则可认为让误差的平方和最小的 y 为真实值，即

$$\left| y-y_i \right| \to (y-y_i)^2\text{ minimum}\Rightarrow \text{true } y $$

这就是**最小二乘法**，在一元线性回归的场景下，若 y 与 x 看起来像是线性的关系，假定为

$$y = f(x) = ax+b$$

则误差的平方和可表示为

$$\sum (f(x_i)-y_i)^2=\sum (ax_i+b-y_i)^2$$

对于上述的误差平方和，可知当

$$\left\{
\begin{aligned}
&\frac {\partial \sum(f(x_i)-y_i)^2}{\partial a}=2\sum(ax_i+b-y_i)x_i=0\\
&\frac {\partial \sum(f(x_i)-y_i)^2}{\partial b}=2\sum(ax_i+b-y_i)=0
\end{aligned}\right.$$

时误差平方和最小，故解出方程组即可回归出 $\hat a$ 和 $\hat b$，则 $y = \hat ax+\hat b$.   
  
以最小二乘法为基础，在 MATLAB 中，常使用 `polyfit` 或 `regress` 进行线性拟合，`polyfit` 为多项式拟合函数，`regress` 为一元或多元线性回归函数。

```matlab
%MATLAB CODE
x = […]; %向量
y = […]; %向量
p = polyfit(x, y, 1); 
%1 次函数拟合 x 和 y，p 返回由高次到低次的系数
X = [ones(size(y)) x];

[b, bint, r, ring, stats] = regress(y, X, alpha); 
%b 回归系数，bint 回归区间估计，r 参差，rint 置信区间，stats 检验的统计量，alpha 显著性水平
```

### 多元线性回归（Linear Regression with Multiple Variables）

与单变量的线性回归不同，多元线性回归运用与含有多个变量的回分析，模型如下

$$\begin{aligned}
&x = (x_1,x_2,…,x_n)\\
&y_i = f(x) = \beta_0+\beta_1 x_1+\beta_2 x_2+…+\beta_n x_n
\end{aligned}$$

同样使用**最小二乘法**可回归出各项的系数 $\hat {\beta_i}$.  
在 MATLAB 中使用函数 `regress` 实现

```matlab
%MATLAB CODE
x1 = […];
x2 = […];
%……
%xn = […];
y = […];
X = [ones(size(y)) x1.^2 x2.^2 x1 x2 x1.*x2]
[b, bint, r, ring, stats] = regress(y, X, alpha)
```

**梯度下降法**  
对于多元线性回归，若使用最小二乘法，则最后要解多个方程确定各项的系数，故在最小二乘法的基础上使用梯度下降法求解方程组。  
假设为二元线性回归

$$
\begin{aligned}
&x = (x1,x2) \\
&y = f(x) = \beta_0+\beta_1 x_1+\beta_2 x_2
\end{aligned}$$
引入代价函数(Cost Function)
$$J(\beta_0,\beta_1,\beta_2) = \frac{1}{2m} \sum_{i=1}^m(f(x_i)-y_i)^2$$

故当代价函数最小时，误差平方和最小，使用梯度下降的方法求代价函数的最小值。  
梯度是指向函数某点处上升最快的方向的向量，故在沿着此方向的反方向前进可以最快速地到达最小值，假设每次前进 $\alpha$ 个步长，则对于每一项系数 $\beta_i$ 每次下降

$$\beta_j:=\beta_j-\alpha \frac{\partial}{\partial \beta_j}J(\beta_0,\beta_1,\beta_2)$$

重复此过程一直到梯度小于 $e^{-5}$，可认为下降到了最小值。  

```matlab

```

### 非线性回归

x 与 y 的关系看起来不像是线性关系，而是曲线的形状，常见的有指数形式和对数形式

$$\begin{aligned}
&y = ax^b\\
&y = a+blnx
\end{aligned}$$

在 MATLAB 中使用函数 `nlinfit` 实现

```matlab
nlinfit()
```

### 逻辑回归（Logistic Regression）

在 MATLAB 中，逻辑回归可以使用 `glmfit` 函数实现

```matlab
[b, dev, stats] = glmfit(X, y, distr, Name, Value)
%
```

### 朴素贝叶斯（Naive Bayes）

### 决策树（Decision Tree）

### 随机森林（Random Forest）

### 支持向量机（Support Vector Machine, SVM）

支持向量机适合对**多维**数据进行预测。  

**SVM 分类**

```matlab
fitcsvm()
```

**SVM 回归**

```matlab
fitrsvm()
```

### 神经网络（Neural Network）

神经网络适合对**大规模**数据进行预测。  


## 无监督学习

### 聚类（Clustering）

**层次聚类**

```matlab

```

**K-means**

```matlab

```

**K-means++**

```matlab

```

### 降维（Dimensionality Reduction）

**主成分分析**（Principal Components Analysis, PCA）

```matlab

```

**因子分析**

```matlab

```

# 启发式算法

## 蒙特卡洛算法


## 遗传算法（Genatic Algorithm, GA）

```matlab

```

## 模拟退火算法

```matlab

```

## 粒子群优化算法（Particle Swarm Optimization,PSO）

```matlab

```

## 蚁群算法

